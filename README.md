# QwenGround: é›¶-Shot 3Dåœºæ™¯ç†è§£å’Œå®šä½ç³»ç»Ÿ

åŸºäº [SeeGround](https://github.com/iris0329/SeeGround) æ€è·¯ï¼Œä½¿ç”¨ Qwen2-VL-7B-Instruct å®ç°çš„é›¶-shotå¼€æ”¾è¯æ±‡3Dè§†è§‰å®šä½ç³»ç»Ÿã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

QwenGround èƒ½å¤Ÿä»è§†é¢‘æˆ–å›¾åƒåºåˆ—ä¸­ç†è§£3Dåœºæ™¯ï¼Œå¹¶æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å®šä½ç›®æ ‡ç‰©ä½“çš„3Dè¾¹ç•Œæ¡†ã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ¯ **é›¶-Shotå®šä½**: æ— éœ€3Dæ ‡æ³¨ï¼Œæ”¯æŒä»»æ„è‡ªç„¶è¯­è¨€æè¿°
- ğŸ¥ **å¤šè¾“å…¥æ¨¡å¼**: æ”¯æŒè§†é¢‘æ–‡ä»¶å’Œå›¾åƒåºåˆ—
- ğŸ”„ **2Dåˆ°3Dé‡å»º**: ä»2Dè¾“å…¥è‡ªåŠ¨é‡å»ºç²—ç³™3Dåœºæ™¯
- ğŸ§  **å¤§æ¨¡å‹é©±åŠ¨**: åŸºäºQwen2-VLçš„å¼ºå¤§è§†è§‰-è¯­è¨€ç†è§£èƒ½åŠ›
- ğŸ“¦ **3Då¯è§†åŒ–**: è¾“å‡ºäº¤äº’å¼3Dåœºæ™¯å’Œè¾¹ç•Œæ¡†

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
è¾“å…¥ (è§†é¢‘/å›¾åƒ) + è‡ªç„¶è¯­è¨€æŸ¥è¯¢
    â†“
[1] Perspective Adaptation Module (å…³é”®å¸§æå–/å¤šè§†è§’ç”Ÿæˆ)
    â†“
[2] 3D Reconstruction (SfMç‚¹äº‘ / æ·±åº¦ä¼°è®¡)
    â†“
[3] Object Lookup Table (ç‰©ä½“æ£€æµ‹å’Œ3Dè¾¹ç•Œæ¡†)
    â†“
[4] Fusion Alignment Module (VLM + 2D-3Dèåˆ)
    â†“
è¾“å‡º (3Dè¾¹ç•Œæ¡†JSON + å¯è§†åŒ–)
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.10+
- CUDA 11.8+ (GPUæ¨è: RTX 3090/4090 æˆ– A100)
- 16GB+ VRAM (7Bæ¨¡å‹) æˆ– 80GB+ (72Bæ¨¡å‹)

### å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
cd /Users/starryyu/Documents/tinghua/QwenGround

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# (å¯é€‰) å®‰è£…COLMAPç”¨äºæ›´å¥½çš„SfM
# Ubuntu: sudo apt-get install colmap
# macOS: brew install colmap
```

### ä½¿ç”¨ç¤ºä¾‹

#### 1. è§†é¢‘è¾“å…¥

```bash
python qwenground_main.py \
    --input_type video \
    --input_path ./examples/kitchen_scene.mp4 \
    --query "the red apple on the wooden table" \
    --output_dir ./outputs/kitchen
```

#### 2. å›¾åƒåºåˆ—è¾“å…¥

```bash
python qwenground_main.py \
    --input_type images \
    --input_path ./examples/room_images/ \
    --query "the laptop near the window" \
    --output_dir ./outputs/room
```

#### 3. Python API

```python
from qwenground import QwenGroundSystem

# åˆå§‹åŒ–ç³»ç»Ÿ
system = QwenGroundSystem(
    model_name="Qwen/Qwen2-VL-7B-Instruct",
    device="cuda"
)

# è¿è¡Œæ¨ç†
result = system.run(
    input_path="path/to/video.mp4",
    query="the red apple on the wooden table",
    input_type="video"
)

print(f"3D BBox: {result['3d_bbox']}")
print(f"Confidence: {result['confidence']}")
```

## ğŸ“Š è¾“å‡ºæ ¼å¼

### JSONè¾“å‡º

```json
{
    "target_object": "apple",
    "anchor_object": "table",
    "3d_bbox": [0.5, 1.2, 0.8, 0.15, 0.15, 0.12],
    "confidence": 0.95,
    "bbox_format": "xyzwhd",
    "point_cloud": "output_scene.ply",
    "visualization": "output_3d_bbox.ply",
    "metadata": {
        "num_frames": 15,
        "num_objects": 8,
        "processing_time": 12.3
    }
}
```

### å¯è§†åŒ–æ–‡ä»¶

- `output_scene.ply`: é‡å»ºçš„3Dç‚¹äº‘
- `output_3d_bbox.ply`: å¸¦è¾¹ç•Œæ¡†çš„3Dåœºæ™¯
- `output_animation.gif`: å¤šè§†è§’æ—‹è½¬åŠ¨ç”»

## ğŸ”§ é…ç½®é€‰é¡¹

### æ¨¡å‹é…ç½®

```yaml
# config/default.yaml
model:
  name: "Qwen/Qwen2-VL-7B-Instruct"
  device: "cuda"
  tensor_parallel_size: 1
  max_tokens: 512

reconstruction:
  keyframe_count: 15
  depth_model: "DPT_Large"
  min_confidence: 0.3

detection:
  yolo_model: "yolov8x.pt"
  iou_threshold: 0.5
  conf_threshold: 0.25
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
QwenGround/
â”œâ”€â”€ qwenground_main.py          # ä¸»å…¥å£
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ perspective_adapter.py  # è§†è§’é€‚åº”æ¨¡å—
â”‚   â”œâ”€â”€ reconstruction_3d.py    # 3Dé‡å»ºæ¨¡å—
â”‚   â”œâ”€â”€ fusion_alignment.py     # èåˆå¯¹é½æ¨¡å—
â”‚   â”œâ”€â”€ object_lookup_table.py  # ç‰©ä½“æŸ¥æ‰¾è¡¨
â”‚   â””â”€â”€ visualization.py        # å¯è§†åŒ–æ¨¡å—
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ vlm_client.py           # VLMå®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ depth_estimation.py     # æ·±åº¦ä¼°è®¡
â”‚   â””â”€â”€ helpers.py              # è¾…åŠ©å‡½æ•°
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml            # é»˜è®¤é…ç½®
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“ æŠ€æœ¯ç»†èŠ‚

### Perspective Adaptation Module

- ä»è§†é¢‘æå–10-20ä¸ªå…³é”®å¸§ï¼ˆåŸºäºåœºæ™¯å˜åŒ–ï¼‰
- å¯¹å›¾åƒåºåˆ—è¿›è¡Œè§†è§’åˆ†æå’Œé€‰æ‹©
- æ ¹æ®æŸ¥è¯¢å†…å®¹ä¼˜å…ˆé€‰æ‹©ç›¸å…³è§†è§’

### 3D Reconstruction

- **è§†é¢‘æ¨¡å¼**: OpenCVå…³é”®å¸§ â†’ Open3D SfM â†’ ç¨€ç–ç‚¹äº‘
- **å›¾åƒæ¨¡å¼**: MiDaSæ·±åº¦ä¼°è®¡ â†’ ä¼ª3Dåæ ‡ç”Ÿæˆ
- ä½¿ç”¨YOLOv8è¿›è¡Œ2Dç‰©ä½“æ£€æµ‹
- å°†2Dè¾¹ç•Œæ¡†æŠ•å½±åˆ°3Dç©ºé—´

### Fusion Alignment

- ä½¿ç”¨Qwen-VLè§£ææŸ¥è¯¢ï¼ˆæå–targetå’Œanchorï¼‰
- åœ¨2Dè§†å›¾ä¸Šæ·»åŠ è§†è§‰æç¤ºï¼ˆå½©è‰²æ¡†ï¼‰
- ç”Ÿæˆç©ºé—´å…³ç³»æè¿°ï¼ˆ"Xä½äºYçš„å·¦ä¸Šæ–¹"ï¼‰
- VLMè¿›è¡Œ2D grounding â†’ æ˜ å°„åˆ°3Dåæ ‡

### Object Lookup Table

```python
OLT = {
    'object_id': 1,
    'class_name': 'apple',
    '2d_bbox': [x1, y1, x2, y2],
    '3d_bbox': [x, y, z, w, h, d],
    'confidence': 0.95,
    'frame_ids': [0, 3, 7],
    'embedding': np.array([...])
}
```

## ğŸ”¬ æ€§èƒ½ä¼˜åŒ–

- ä½¿ç”¨vLLMåŠ é€ŸVLMæ¨ç†ï¼ˆ2-3xåŠ é€Ÿï¼‰
- å…³é”®å¸§é‡‡æ ·å‡å°‘è®¡ç®—é‡
- å¤šGPUå¹¶è¡Œï¼ˆè®¾ç½®`tensor_parallel_size`ï¼‰
- æ··åˆç²¾åº¦æ¨ç†ï¼ˆFP16ï¼‰

## âš ï¸ å·²çŸ¥é™åˆ¶

- å•ç‰©ä½“åœºæ™¯æ•ˆæœæœ€ä½³
- éœ€è¦ä¸­ç­‰è´¨é‡çš„è¾“å…¥ï¼ˆåˆ†è¾¨ç‡â‰¥720pï¼‰
- å¿«é€Ÿè¿åŠ¨åœºæ™¯å¯èƒ½å½±å“é‡å»ºè´¨é‡
- é€æ˜/åå…‰ç‰©ä½“å¯èƒ½å®šä½ä¸å‡†

## ğŸ“ å¼•ç”¨

```bibtex
@inproceedings{qwenground2025,
  title={QwenGround: Zero-Shot 3D Visual Grounding with Vision-Language Models},
  author={Your Name},
  year={2025}
}

@inproceedings{li2025seeground,
  title={SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding},
  author={Li et al.},
  booktitle={CVPR},
  year={2025}
}
```

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

