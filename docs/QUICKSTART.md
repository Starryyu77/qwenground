# QwenGround å¿«é€Ÿå…¥é—¨

5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹ä½¿ç”¨QwenGroundï¼

## å‰ç½®æ¡ä»¶

- Python 3.10+
- CUDA 11.8+ (æ¨èä½¿ç”¨GPU)
- è‡³å°‘16GB VRAM

## å®‰è£…

```bash
# 1. è¿›å…¥é¡¹ç›®ç›®å½•
cd /Users/starryyu/Documents/tinghua/QwenGround

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n qwenground python=3.10
conda activate qwenground

# 3. å®‰è£…PyTorch (CUDA 11.8)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# 4. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 5. æµ‹è¯•å®‰è£…
python scripts/test_installation.py
```

## ç¬¬ä¸€æ¬¡è¿è¡Œ

### ç¤ºä¾‹1: è§†é¢‘è¾“å…¥

```bash
python qwenground_main.py \
    --input_type video \
    --input_path ./path/to/your/video.mp4 \
    --query "the red apple on the table" \
    --output_dir ./outputs/test1
```

### ç¤ºä¾‹2: å›¾åƒåºåˆ—

```bash
python qwenground_main.py \
    --input_type images \
    --input_path ./path/to/images/ \
    --query "the laptop near the window" \
    --output_dir ./outputs/test2
```

## è¾“å‡ºæ–‡ä»¶

è¿è¡Œå®Œæˆåï¼Œåœ¨è¾“å‡ºç›®å½•ä¸­ä¼šçœ‹åˆ°ï¼š

```
outputs/test1/
â”œâ”€â”€ result.json              # ä¸»è¦ç»“æœï¼ˆJSONæ ¼å¼ï¼‰
â”œâ”€â”€ pointcloud.ply          # 3Dç‚¹äº‘
â”œâ”€â”€ scene_with_bbox.ply     # å¸¦è¾¹ç•Œæ¡†çš„åœºæ™¯
â”œâ”€â”€ animation.gif           # æ—‹è½¬åŠ¨ç”»
â”œâ”€â”€ summary.png             # ç»“æœæ‘˜è¦å›¾
â””â”€â”€ result_images/          # æ ‡æ³¨åçš„å…³é”®å¸§
    â”œâ”€â”€ result_frame_0000.jpg
    â”œâ”€â”€ result_frame_0003.jpg
    â””â”€â”€ ...
```

### æŸ¥çœ‹ç»“æœ

```bash
# æŸ¥çœ‹JSONç»“æœ
cat outputs/test1/result.json

# ä½¿ç”¨Open3DæŸ¥çœ‹ç‚¹äº‘
python -c "import open3d as o3d; pcd = o3d.io.read_point_cloud('outputs/test1/pointcloud.ply'); o3d.visualization.draw_geometries([pcd])"
```

## Python API ä½¿ç”¨

```python
from qwenground_system import QwenGroundSystem

# åˆå§‹åŒ–ç³»ç»Ÿ
system = QwenGroundSystem(
    model_name="Qwen/Qwen2-VL-7B-Instruct",
    device="cuda"
)

# è¿è¡Œæ¨ç†
result = system.run(
    input_path="path/to/video.mp4",
    query="the red apple on the wooden table",
    input_type="video",
    output_dir="./outputs/demo"
)

# æŸ¥çœ‹ç»“æœ
if result['success']:
    print(f"ç›®æ ‡ç‰©ä½“: {result['target_object']}")
    print(f"3Dè¾¹ç•Œæ¡†: {result['3d_bbox']}")
    print(f"ç½®ä¿¡åº¦: {result['confidence']}")
```

## ä½¿ç”¨vLLM APIæ¨¡å¼ï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

### 1. å¯åŠ¨vLLMæœåŠ¡å™¨

```bash
# æ¨èï¼šé€šè¿‡ç¯å¢ƒå˜é‡æä¾›å¯†é’¥
export API_KEY="<your_api_key>"
bash scripts/deploy_vllm.sh
```

æˆ–æ‰‹åŠ¨å¯åŠ¨ï¼š

```bash
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2-VL-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --api-key "$API_KEY"
```

### 2. ä½¿ç”¨APIæ¨¡å¼è¿è¡Œ

```bash
python qwenground_main.py \
    --use_api \
    --api_url http://localhost:8000/v1 \
    --api_key "$API_KEY" \
    --input_path ./video.mp4 \
    --query "the person on the left"
```

## é…ç½®é€‰é¡¹

### ä½¿ç”¨é…ç½®æ–‡ä»¶

åˆ›å»ºè‡ªå®šä¹‰é…ç½®ï¼š

```yaml
# config/my_config.yaml
reconstruction:
  keyframe_count: 20
  depth_model: "DPT_Large"

detection:
  yolo_model: "yolov8x.pt"
  conf_threshold: 0.3
```

è¿è¡Œï¼š

```bash
python qwenground_main.py \
    --config config/my_config.yaml \
    --input_path ./video.mp4 \
    --query "..."
```

### å¸¸ç”¨å‚æ•°

```bash
# è°ƒæ•´å…³é”®å¸§æ•°é‡ï¼ˆæ›´å¤šå¸§=æ›´å‡†ç¡®ä½†æ›´æ…¢ï¼‰
python qwenground_main.py ... --config config/default.yaml

# ä¸ç”Ÿæˆå¯è§†åŒ–ï¼ˆåŠ é€Ÿï¼‰
python qwenground_main.py ... --no_visualize

# ä¿å­˜ä¸­é—´ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
python qwenground_main.py ... --save_intermediate

# è°ƒæ•´æ—¥å¿—çº§åˆ«
python qwenground_main.py ... --log_level DEBUG
```

## æ€§èƒ½ä¼˜åŒ–

### å¯¹äºè¾ƒæ…¢çš„æœºå™¨

```yaml
# config/fast.yaml
reconstruction:
  keyframe_count: 8           # å‡å°‘å¸§æ•°
  depth_model: "MiDaS_small"  # ä½¿ç”¨å°æ¨¡å‹

detection:
  yolo_model: "yolov8m.pt"    # ä½¿ç”¨ä¸­ç­‰æ¨¡å‹
  conf_threshold: 0.35        # æé«˜é˜ˆå€¼
```

### å¯¹äºé«˜æ€§èƒ½æœºå™¨

```yaml
# config/quality.yaml
reconstruction:
  keyframe_count: 25          # æ›´å¤šå¸§
  depth_model: "DPT_Large"    # æ›´å¥½çš„æ¨¡å‹

detection:
  yolo_model: "yolov8x.pt"    # æœ€å¤§æ¨¡å‹
  conf_threshold: 0.2         # æ›´ä½é˜ˆå€¼
```

## å¸¸è§æŸ¥è¯¢ç¤ºä¾‹

```bash
# ç©ºé—´å…³ç³»æŸ¥è¯¢
--query "the apple on the table"
--query "the book next to the lamp"
--query "the chair behind the desk"

# å±æ€§æŸ¥è¯¢
--query "the red apple"
--query "the wooden table"
--query "the black laptop"

# ç»„åˆæŸ¥è¯¢
--query "the red apple on the wooden table"
--query "the person sitting on the blue chair"
--query "the bottle to the left of the cup"
```

## æ•…éšœæ’é™¤

### GPUå†…å­˜ä¸è¶³

```bash
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
python qwenground_main.py \
    --model_name Qwen/Qwen2-VL-2B-Instruct \
    ...
```

### è§†é¢‘å¤ªå¤§

```bash
# åœ¨å¤–éƒ¨å…ˆé™ä½åˆ†è¾¨ç‡
ffmpeg -i input.mp4 -vf scale=1280:720 output.mp4
```

### æ·±åº¦æ¨¡å‹ä¸‹è½½æ…¢

```bash
# é¢„å…ˆä¸‹è½½
python -c "import torch; torch.hub.load('intel-isl/MiDaS', 'MiDaS_small')"
```

## ä¸‹ä¸€æ­¥

- æŸ¥çœ‹ `examples/example_usage.py` äº†è§£æ›´å¤šAPIç¤ºä¾‹
- é˜…è¯» `INSTALL.md` äº†è§£è¯¦ç»†å®‰è£…è¯´æ˜
- é˜…è¯» `README.md` äº†è§£ç³»ç»Ÿæ¶æ„
- ä¿®æ”¹ `config/default.yaml` è‡ªå®šä¹‰é…ç½®

## è·å–å¸®åŠ©

```bash
# æŸ¥çœ‹å®Œæ•´å¸®åŠ©
python qwenground_main.py --help

# è¿è¡Œæµ‹è¯•
python scripts/test_installation.py
```

---

ğŸ‰ äº«å—ä½¿ç”¨QwenGroundï¼

